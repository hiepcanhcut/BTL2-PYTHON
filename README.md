# ğŸ“Š BÃ i Táº­p Lá»›n Sá»‘ 2 â€” MÃ´n Python ğŸ

**TÃªn Ä‘á» tÃ i:** Nháº­n dáº¡ng áº£nh CIFAR-10 báº±ng Máº¡ng NÆ¡-ron TÃ­ch Cháº­p (CNN 3 lá»›p)

## ğŸ“‘ Má»¥c lá»¥c

ğŸ“ Report/  
ğŸ“„ BÃ¡o-cÃ¡o.pdf # BÃ¡o cÃ¡o tá»•ng há»£p káº¿t quáº£ bÃ i táº­p lá»›n

ğŸ“ SourceCode/  
ğŸ“ CNN_Res/ # Káº¿t quáº£ huáº¥n luyá»‡n: Loss Curve, Accuracy Curve, Confusion Matrix  
ğŸ“ CNN_Code/ # Code

ğŸ“„ README.md # File mÃ´ táº£ dá»± Ã¡n

## ğŸ“ MÃ” Táº¢ Dá»° ÃN

Dá»± Ã¡n nÃ y táº­p trung vÃ o **xÃ¢y dá»±ng vÃ  huáº¥n luyá»‡n má»™t máº¡ng CNN gá»“m 3 lá»›p convolution** Ä‘á»ƒ phÃ¢n loáº¡i áº£nh trÃªn táº­p dá»¯ liá»‡u **CIFAR-10** (60.000 áº£nh mÃ u 32Ã—32, thuá»™c 10 lá»›p).  
Trong quÃ¡ trÃ¬nh thá»±c hiá»‡n, mÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c cáº£i tiáº¿n vá»›i cÃ¡c ká»¹ thuáº­t nhÆ° **Batch Normalization**, **Dropout**, **Data Augmentation** vÃ  **Learning-rate Scheduler** nháº±m nÃ¢ng cao Ä‘á»™ chÃ­nh xÃ¡c vÃ  giáº£m hiá»‡n tÆ°á»£ng overfitting.

---

## ğŸ“Œ CÃC BÆ¯á»šC THá»°C HIá»†N

- **BÆ°á»›c 1:** Äá»‹nh nghÄ©a kiáº¿n trÃºc CNN  
  - XÃ¢y dá»±ng má»™t máº¡ng CNN gá»“m 3 khá»‘i convolution:  
    > - Má»—i khá»‘i: Conv2d â†’ BatchNorm2d â†’ ReLU â†’ MaxPool2d  
    > - Sá»‘ kÃªnh tÄƒng dáº§n qua cÃ¡c lá»›p: 3 â†’ 64 â†’ 128 â†’ 256  
    > - ThÃªm lá»›p Dropout (p=0.5) trÆ°á»›c fully connected Ä‘á»ƒ giáº£m overfitting  
  - MÃ£ lá»‡nh Ä‘á»‹nh nghÄ©a model vÃ  giáº£i thÃ­ch tá»«ng thÃ nh pháº§n.

- **BÆ°á»›c 2:** Chuáº©n bá»‹ dá»¯ liá»‡u vá»›i Data Augmentation  
  - Sá»­ dá»¥ng **RandomCrop(padding=4)** vÃ  **RandomHorizontalFlip** trÃªn táº­p train Ä‘á»ƒ tÄƒng tÃ­nh Ä‘a dáº¡ng.  
  - Chá»‰ Ã¡p dá»¥ng **ToTensor()** vÃ  **Normalize** (mean = [0.4914, 0.4822, 0.4465], std = [0.2023, 0.1994, 0.2010]) cho train/validation/test.

- **BÆ°á»›c 3:** Huáº¥n luyá»‡n vÃ  Validate mÃ´ hÃ¬nh  
  - Khá»Ÿi táº¡o model lÃªn device (GPU náº¿u cÃ³).  
  - Sá»­ dá»¥ng **CrossEntropyLoss** lÃ m hÃ m máº¥t mÃ¡t, **Adam** lÃ m optimizer (learning rate = 1e-3).  
  - Ãp dá»¥ng **StepLR** (gamma = 0.5, step_size = 10) Ä‘á»ƒ giáº£m learning rate má»—i 10 epoch.  
  - Huáº¥n luyá»‡n trong **25 epoch**:  
    > 1. Chuyá»ƒn `model.train()`, láº·p qua tá»«ng batch, tÃ­nh toÃ¡n loss, backward vÃ  optimizer.step().  
    > 2. Chuyá»ƒn `model.eval()` trÃªn táº­p validation, khÃ´ng tÃ­nh gradient, tÃ­nh `val_loss` vÃ  `val_acc`.  
    > 3. LÆ°u lá»‹ch sá»­ `train_loss`, `val_loss`, `train_acc`, `val_acc` vÃ o cáº¥u trÃºc `history`.

- **BÆ°á»›c 4:** ÄÃ¡nh giÃ¡ trÃªn táº­p Test vÃ  trá»±c quan hÃ³a  
  - Sau khi huáº¥n luyá»‡n xong, chuyá»ƒn `model.eval()` vÃ  cháº¡y dá»± Ä‘oÃ¡n trÃªn toÃ n bá»™ test set.  
  - TÃ­nh **Test Accuracy**.  
  - Váº½ **Loss Curve** (Train vs. Validation) vÃ  **Accuracy Curve** (Train vs. Validation) báº±ng matplotlib.  
  - Sá»­ dá»¥ng **confusion_matrix** tá»« scikit-learn Ä‘á»ƒ táº¡o ma tráº­n nháº§m láº«n, sau Ä‘Ã³ hiá»ƒn thá»‹ báº±ng **ConfusionMatrixDisplay**.  
  - LÆ°u ba hÃ¬nh áº£nh:  
    > - `Loss Curve.png`  
    > - `Accuracy Curve.png`  
    > - `Confusion Matrix.png`  

---

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c chi tiáº¿t

1. **`Report/`**  
   - Bao gá»“m file PDF â€œBÃ¡o-cÃ¡o.pdfâ€ lÃ  bÃ¡o cÃ¡o tá»•ng há»£p toÃ n bá»™ quÃ¡ trÃ¬nh triá»ƒn khai, giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c, káº¿t quáº£ thá»±c thi vÃ  nháº­n xÃ©t, Ä‘á» xuáº¥t cáº£i tiáº¿n.

2. **`SourceCode/CNN-code.py`**  
   - File Python chÃ­nh, chá»©a táº¥t cáº£ cÃ¡c bÆ°á»›c tá»« Ä‘á»‹nh nghÄ©a kiáº¿n trÃºc, chuáº©n bá»‹ dá»¯ liá»‡u, huáº¥n luyá»‡n, validate, Ä‘áº¿n Ä‘Ã¡nh giÃ¡ vÃ  lÆ°u hÃ¬nh áº£nh káº¿t quáº£.

3. **`SourceCode/CNN_Res/`**  
   - **`Loss Curve.png`**: Äá»“ thá»‹ thá»ƒ hiá»‡n giÃ¡ trá»‹ loss cá»§a train vÃ  validation qua 25 epoch.  
   - **`Accuracy Curve.png`**: Äá»“ thá»‹ thá»ƒ hiá»‡n giÃ¡ trá»‹ accuracy cá»§a train vÃ  validation qua 25 epoch.  
   - **`Confusion Matrix.png`**: Ma tráº­n nháº§m láº«n trÃªn táº­p test, trá»±c quan má»©c Ä‘á»™ Ä‘Ãºng/sai giá»¯a 10 lá»›p CIFAR-10.

4. **`SourceCode/README.md`**  
   - File nÃ y (README.md) mÃ´ táº£ chi tiáº¿t cáº¥u trÃºc thÆ° má»¥c vÃ  hÆ°á»›ng dáº«n cháº¡y project.

---

